---
title: "MSAN 601 Case Study"
author: "Alvira Swalin, Kunal Kotian, Sooraj Subrahmannian, Vinay Patlolla"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include = FALSE, echo = F, message=FALSE, warning=F}
# Prevent comments from running off the page (force line-wrapping for comments)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 70), tidy = TRUE, cache = T)
```

# Introduction

This report focuses on the analysis of the value of residential homes in Ames, Iowa.  Linear regression techniques were used to address the following key questions:

-	What aspects of a house in Ames, IA most strongly influence its value?
-	How can we predict house prices in Ames, IA?

The report is divided into 3 main sections.  Section 1 describes the process of cleaning the housing data, Section 2 describes how an explanatory model was built for the housing data, and finally Section 3 focuses on predictive modeling for estimating housing prices.

```{r message=F, warning=F, echo=F}
# Load required packages
library(tidyverse)
library(magrittr)
library(glmnet)
library(ISLR)
library(mice)
library(MASS)
library(car)
library(VIM)
library(corrplot)
set.seed(1)
```

```{r echo=F, warning=FALSE, message=FALSE}
# Load housing data
housing <- read.csv('housing.txt', stringsAsFactors = FALSE)
Mortydata <- read.csv('Morty.txt',stringsAsFactors = F)
housing <- rbind(housing,Mortydata[,-1])
housing <- tbl_df(housing)
```

# Cleaning the Housing Dataset

## Missing Values

The data loaded from `housing.txt` contained several missing values (`NA`s).  Table 1 shows a summary of the proportion of `NA`s in each column.  Figure 1 shows a visual overview of the missing data.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Counting ratio of NA's in each column
na_count <- ((colSums(is.na(housing))/nrow(housing)) * 100) %>% round(1)
# Summarising the columns which have NA count > 0
# Table 1
na_df <- data.frame(sort(na_count[na_count > 0], decreasing = T))
colnames(na_df) <- c('percentage')
knitr::kable(na_df, caption = "% of NAs before cleaning", col.names = c("NAs (%)"))
```


```{r fig1, fig.cap="Overview of the missing data", echo=F, message=FALSE, warning=F}
mice_plot <- aggr(housing, col = c('navyblue','red'),
                  numbers = T, sortVars = TRUE, 
                  labels = names(housing), cex.axis = .7,
                  gap = 3, ylab = c("Missing data","Pattern"), combined = F)
```


### Missing Values Which Are Not Really Missing

In the case of several categorical variables, the data dictionary stated that `NA` represented a specific state/condition denoting the absence of the entity tracked by the categorical variable.  Such `NA` values are not truly missing values.  Hence, we replaced these instances of `NA`s with values denoting the state of absence of the entities tracked by the categorical variables - e.g. the value `NoBasement` indicates the absence of a basement.


```{r echo=F, warning=F, message=F}
# Defining function clean data which cleans the dataset
# based on the description
# For example: In Basement Quality column, NA represents 'No Basement' which is represented
# as a new level for that category. In cases like MasVnrType column, where the data is actually
# we defined the level as 'Missing Veneer Type'
clean_data <- function(df){
  df$Id <- NULL
  df$PoolQC[is.na(df$PoolQC)] <- 'No Pool'
  df$Alley[is.na(df$Alley)] <- 'No Alley'
  df$MiscFeature[is.na(df$MiscFeature)] <- 'None'
  df$Fence[is.na(df$Fence)] <- 'No Fence'
  df$FireplaceQu[is.na(df$FireplaceQu)] <- 'No Fireplace'
  #df$LotFrontage[is.na(df$LotFrontage)] <- 'No Lot'
  df$GarageType[is.na(df$GarageType)] <- 'No Garage'
  #df$GarageYrBlt[is.na(df$GarageYrBlt)] <- 'No Garage'
  #df$GarageFinish[is.na(df$GarageFinish)] <- 'No Garage'
  df$GarageQual[is.na(df$GarageQual)] <- 'No Garage'
  df$GarageCond[is.na(df$GarageCond)] <- 'No Garage'
  df$BsmtExposure[is.na(df$BsmtExposure)] <- 'No Basement'
  df$BsmtQual[is.na(df$BsmtQual)] <- 'No Basement'
  df$BsmtCond[is.na(df$BsmtCond)] <- 'No Basement'
  df$BsmtFinType1[is.na(df$BsmtFinType1)] <- 'No Basement'
  df$BsmtFinType2[is.na(df$BsmtFinType2)] <- 'No Basement'
  df$MasVnrType[is.na(df$MasVnrType)] <- 'Missing Masonry Veneer'
  df$Electrical[is.na(df$Electrical)] <- 'Missing Electrical'
  return(df)
}
```

### Dealing with Missing Values

Table 2 shows a summary of the missing values left in the dataset after the `NA`-replacement process described here was completed.  The missing values in variables `LotFrontage` and `MasVnrArea` were imputed using the package `mice`.

`NA`s in `GarageYrBlt` and `GarageFinish` represent the cases in which a house does not have any garage. Imputing such missing values does not make sense i.e there is no proper way to represent these observations.  Hence, all rows with `NA`s in `GarageYrBlt` or `GarageFinish` were deleted.  Thus, we end up with a dataset that has no missing values.

```{r echo=FALSE, warning=F, message=F}
# Applying clean data function on housing data set
housing <- clean_data(housing)
# Summarising NA ratio after cleaning
na_count <- ((colSums(is.na(housing))/nrow(housing)) * 100) %>% round(1)
# Table 2
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs after cleaning", col.names = c("NAs (%)"))
```

The variables were further analysed for ordered categorical variables. We found that some variables indicating quality or condition made more sense when they where ordered. For example, the variable `ExterQual` consisted of multiple levels such as 'Excellent', 'Poor' etc.  Such variables need to be considered as ordinal factors as there is an inherent order.

```{r echo=FALSE, warning=F, message=F}
# We need to impute data for missing values in columns like LotFrontage and MasVnrArea
# Also removing observations which have NA's in GarageYrBlt because NA's 
# here represent that house does not have any garage and imputing here does 
# not make sense i.e there is no proper way to represent these observations.
impute_data <- function(df){
  ## Convert character columns to factors
  df_imputed <- as.data.frame(unclass(df))
  # Columns(ExterQual, ExterCond, HeatingQC, KitchenQual) which have a 
  # inherent order are converted to ordinal factors
  df$ExterQual <- ordered(df$ExterQual,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$ExterCond <- ordered(df$ExterCond,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$HeatingQC <- ordered(df$HeatingQC,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$KitchenQual <- ordered(df$ExterCond,
                            levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  ## Removing the observations which have NA values in GarageYrBlt
  df_imputed <- df_imputed[!is.na(df_imputed$GarageYrBlt), ]
  ## Imputing missing data with mice using CART method
  df_cleaned <- mice(df_imputed, m = 1, method = 'cart', printFlag = FALSE)
  df_cleaned <- mice::complete(df_cleaned)
  df_cleaned
}
housing_cleaned <- impute_data(housing)
mortydata <- housing_cleaned[nrow(housing_cleaned),]
housing_cleaned <- housing_cleaned[-nrow(housing_cleaned),]
```

```{r echo=FALSE, warning=F, message=F}
# Summarising statistics of each column before and after imputation
summary_area_df <- as.data.frame(unclass(summary(
  housing$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_df) <- "value"
# knitr::kable(summary_area_df, 
#              caption = "MasVnrArea column statistics before imputing")
summary_area_clean_df <- as.data.frame(unclass(summary(
  housing_cleaned$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_clean_df) <- "value"
# knitr::kable(summary_area_clean_df,
#              caption = "MasVnrArea column statistics after imputing")
# Summarising sd of each column before and after imputation
# sd(housing$MasVnrArea[!is.na(housing$MasVnrArea)])
# sd(housing_cleaned$MasVnrArea)
```


## Influential Points

Influential observations are those observations that have a distorting influence on a fitted linear regression model.  Influential points can end up adversely affecting the entire data analysis leading to incorrect conclusions.  Hence, before delving into further analysis, influential observations in the `housing` dataset were detected and removed.


```{r fig2, fig.cap="Influence diagnostics using DfFits", echo=F, message=FALSE, warning=F}
X_influential = model.matrix(SalePrice ~., housing_cleaned)[,-1]
y = housing_cleaned$SalePrice
fit_influential_pts <- lm(y ~ X_influential)
n <- nrow(X_influential)
k <- length(fit_influential_pts$coefficients) - 1
cv <- 2 * sqrt(k/n)
plot(dffits(fit_influential_pts), 
     ylab = "Standardized dfFits", xlab = "Index", 
     main = paste("Standardized DfFits,
                  \n critical value = 2*sqrt(k/n) = +/-",
                  round(cv,3)), ylim = c(-5,5))
abline(h = cv, lty = 2)
abline(h = -cv, lty = 2)
index_influential = which(dffits(fit_influential_pts) > cv | dffits(fit_influential_pts) < -cv)
housing_cleaned = housing_cleaned[-index_influential,]
```

## Wrapping Up Data Clean-up

After the imputation/removal of missing values and removal of influential observations, only one level under `Utilities` was left in the dataset.  Since this made `Utilities` a redundant variable, it was removed altogether from the dataset.

```{r echo=FALSE, warning=F, message=F}
# Checking if any column has only 1 level after removing influential points
count_uniques <- sapply(housing_cleaned, function(x){ length(unique(x)) })
```

```{r echo=FALSE, warning=F, message=F}
# As we can observe Utilities has only 1 level after removing influential points,
# so we can remove the 'Utilities' column from our data set
housing_cleaned$Utilities <- NULL
mortydata$Utilities <- NULL
```

\pagebreak

# Explanatory Model for Housing Prices

The cleaned dataset includes 79 variables.  To develop an explanatory model, it is essential to reduce the number of variables used.  Hence, the Lasso linear regression technique was employed to reduce the number of variables required for the model.

```{r echo=FALSE, warning=F, message=F}
# Model matrix creation 
mortyincluded <- rbind(housing_cleaned,mortydata)
X = model.matrix(SalePrice ~.,mortyincluded)[,-1]
XMorty <- X[nrow(X),]
Xtrainingmodel <- X[-nrow(X),]
XMorty <- t(as.data.frame(XMorty))
```

## Variable Selection Using Lasso Regression

```{r fig3, fig.cap="Variation of MSE with log(Lambda)", echo=F, message=FALSE, warning=F}
# Lasso for prediction
X_lasso <- Xtrainingmodel
yMorty <- mortydata$SalePrice
y_lasso <- housing_cleaned$SalePrice
grid.lambda <- 10^seq(10, -2, length = 1000)

cv.out <- cv.glmnet(X_lasso, y_lasso, alpha = 1)
best.lambda <- cv.out$lambda.1se
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

lasso.model_best.lambda <- glmnet(X_lasso, y_lasso,
                                  alpha = 1, lambda = best.lambda)
cf <- coef(lasso.model_best.lambda)

# New Data Matrix
col_names <- names(cf[-1,])[which(cf[-1,] != 0)]
X_lasso <- X_lasso[, col_names]
XMorty_lasso <- XMorty[, col_names, drop = F]

myDf <- data.frame(y_lasso = y_lasso, X_lasso)
#Fitting OLS Model
ols.fit <- lm(y_lasso~., data=myDf)

Y_pred_Morty <- predict(ols.fit, data.frame(XMorty_lasso),
                        interval="confidence", level = 0.95)
```

Figure 3 shows the variation of the mean squared error (MSE) with log($\lambda$), where $\lambda$ is the tuning parameter that adjusts the effect of the regularization penalty.  Based on the results of Lasso linear regression, the number of variables were reduced to **`r length(col_names)`**, corresponding to the value of $\lambda_{min}$ ($\lambda$ at minimum MSE) + 1 standard error.


## Checking for Normality

The tests and confidence intervals we use are based on the assumption of normal errors. Therefore, the residuals needs to be assessed for normality using a Q-Q plot and Kolmogorov-Smirnoff test for normality. We find that the residuals passes the test. But the Q-Q plot shows heteroskedasticity in residuals.

### Normality Check Before Tranformation

```{r fig4, fig.cap="Residual plot before transformation", echo=FALSE, warning=F, message=F}
# Check for normality before tranformation
resi <- residuals(ols.fit)
fitvalues <- fitted.values(ols.fit)
plot(fitvalues, resi)
stdresi <- scale(resi)
ks.test(stdresi, rnorm(length(stdresi)))
```

```{r fig5, fig.cap="qqplot before transformation", echo=FALSE, warning=F, message=F}
qqnorm(stdresi, ylab = "Residuals")
qqline(stdresi)
```

By doing a box-cox transform, we realised that `sqrt(y)` can be modelled with the design matrix better than `y`.

```{r fig6, fig.cap="boxcox plot", echo=FALSE, warning=F, message=F}
boxcox(ols.fit)
```

### Normality Check After Tranformation
After transforming the y variables, a considerable decrease in heteroscedasticity was seen. The Q-Q plot also shows significant improvement after the transformation indicating the that transformed response variable adhere to normality more than untransformed variable

```{r fig7, fig.cap="Residual plot before transformation", echo=FALSE, warning=F, message=F}
y_sqrt = (y_lasso^0.5)
fit_normality_trans <- lm(y_sqrt ~ X_lasso)
resi <- residuals(fit_normality_trans)
fitvalues_trans <- fitted.values(fit_normality_trans)
fitdf <- data.frame(cbind(fitvalues_trans, resi))
resiplot<- lm(fitvalues_trans ~ resi, data = fitdf)
plot(fitvalues_trans,resi)
stdresi <- scale(resi)
ks.test(stdresi, rnorm(length(stdresi)))
```

```{r fig8, fig.cap="qqplot before transformation", echo=FALSE, warning=F, message=F}
qqnorm(stdresi, ylab = "Residuals")
qqline(stdresi)
```
Since the normality assumption is valid, we can now run an Ordinary least squares on the reduced set of varaibles from Lasso. 
We can then extract a subset of variables which are statistically significant. This model can be used to explain the Salesprice.

```{r}
myDf <- data.frame(y_sqrt = y_sqrt, X_lasso)
#Fitting OLS Model
ols.fit <- lm(y_sqrt~., data=myDf)

significant_predictors <- data.frame(summary(ols.fit)$coef[
  summary(ols.fit)$coef[,4] <= .05, 4])
X_lasso_significant <- X_lasso[, rownames(significant_predictors)[-1]]
```

## Predicting Morty's house value

```{r}
Y_pred_Morty <- predict(ols.fit, data.frame(XMorty_lasso),
                        interval="confidence", level = 0.95)
Y_pred_Morty <- Y_pred_Morty^2
colnames(Y_pred_Morty) <- c('Fit prediction', 'Lower Bound', 'Upper Bound')
knitr::kable(Y_pred_Morty, caption = "95% Confidence Interval of Morty's house")
```

\pagebreak

## Plotting pairwise collinearity
```{r fig9, fig.cap = "Pairwise correlation plot", fig.height = 10, fig.width = 10}
pairwise_cors <- cor(X_lasso)
corrplot(pairwise_cors)
vif_scores <- vif(ols.fit)
vif_scores <- data.frame(vif(ols.fit))
vif_scores['predictors'] <- rownames(vif_scores)
rownames(vif_scores) <- NULL
vif_scores <- vif_scores[, c(2, 1)]
colnames(vif_scores) <- c('predictors', 'scores')
knitr::kable(vif_scores[order(-vif_scores$scores), ],
             caption = "VIF scores of significant predictors")
```

\pagebreak

# Predictive Modelling

## Prediction - Ridge
```{r}
set.seed(48)

X <- Xtrainingmodel
y <- housing_cleaned$SalePrice

grid <- 10^seq(10, -2, length = 100)

#10 Fold Validation Ridge
test <- sample(1:nrow(X), nrow(X)/5)
train <- (-test)
y.train <- y[train]
y.test <- y[test]
  
ridge.mod <- glmnet(X[train,], y.train, alpha = 0)

cv.out <- cv.glmnet(X[train,], y.train, alpha = 0)
best.lambda <- cv.out$lambda.min


ridge.pred <- predict(ridge.mod,s=best.lambda,newx=X[test,])
mean((ridge.pred-y.test)^2)

ridge.pred_morty=predict(ridge.mod,s=best.lambda,newx=XMorty)
Morty_SalePrice = 143000
diff_Morty = Morty_SalePrice - ridge.pred_morty

residuals <- y.test - ridge.pred
MSPE <- mean(residuals^2)
lambda_min <- best.lambda


diff_Morty

```


## Prediction-Elastic Net 
```{r}
diff_Morty <- numeric(10)
MSPE <- numeric(10)
lambda_min <- numeric(10)


grid <- 1:10
test <- sample(1:nrow(X), nrow(X)/5)
train <- (-test)
y.train <- y[train]
y.test <- y[test]
#10 Fold Validation Ridge
j <- 0
for (j in grid){
  alpha <- (j-1)/10
  set.seed(48)

  
  ridge.mod <- glmnet(X[train,], y.train, alpha = alpha)

  cv.out <- cv.glmnet(X[train,], y.train, alpha = alpha)
  best.lambda <- cv.out$lambda.min

  ridge.pred <- predict(ridge.mod,s=best.lambda,newx=X[test,])
  mean((ridge.pred-y.test)^2)

  ridge.pred_morty=predict(ridge.mod,s=best.lambda,newx=XMorty)
  Morty_SalePrice = 143000
  diff_Morty[j] = Morty_SalePrice - ridge.pred_morty

  residuals <- y.test - ridge.pred
  MSPE[j] <- mean(residuals^2)
lambda_min[j] <- best.lambda

}
plot(grid/10,MSPE)
```