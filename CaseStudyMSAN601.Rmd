---
title: "MSAN 601 Case Study"
author: "Alvira Swalin, Kunal Kotian, Sooraj Subrahmannian, Vinay Patlolla"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include = FALSE, echo = TRUE}
# Prevent comments from running off the page (force line-wrapping for comments)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```


```{r message=F}
# Load required packages
library(tidyverse)
library(magrittr)
library(glmnet)
library(ISLR)
library(mice)
library(MASS)
library(car)
set.seed(1)
```

# Cleaning the Dataset

```{r}
# Load housing data
housing <- read.csv('housing.txt', stringsAsFactors = FALSE)
housing <- tbl_df(housing)
```

-----------------------------------------------------------
Vinay


```{r}
# Counting ratio of NA's in each column
na_count <- colSums(is.na(housing))/nrow(housing)
# Summarising the columns which have NA count > 0
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs before cleaning")
```

##Cleaning Data

```{r}
# Defining function clean data which cleans the dataset
# based on the description
# For example: In Basement Quality column, NA represents 'No Basement' which is represented
# as a new level for that category. In cases like MasVnrType column, where the data is actually
# we defined the level as 'Missing Veneer Type'
clean_data <- function(df){
  df$Id <- NULL
  df$PoolQC[is.na(df$PoolQC)] <- 'No Pool'
  df$Alley[is.na(df$Alley)] <- 'No Alley'
  df$MiscFeature[is.na(df$MiscFeature)] <- 'None'
  df$Fence[is.na(df$Fence)] <- 'No Fence'
  df$FireplaceQu[is.na(df$FireplaceQu)] <- 'No Fireplace'
  #df$LotFrontage[is.na(df$LotFrontage)] <- 'No Lot'
  df$GarageType[is.na(df$GarageType)] <- 'No Garage'
  #df$GarageYrBlt[is.na(df$GarageYrBlt)] <- 'No Garage'
  #df$GarageFinish[is.na(df$GarageFinish)] <- 'No Garage'
  df$GarageQual[is.na(df$GarageQual)] <- 'No Garage'
  df$GarageCond[is.na(df$GarageCond)] <- 'No Garage'
  df$BsmtExposure[is.na(df$BsmtExposure)] <- 'No Basement'
  df$BsmtQual[is.na(df$BsmtQual)] <- 'No Basement'
  df$BsmtCond[is.na(df$BsmtCond)] <- 'No Basement'
  df$BsmtFinType1[is.na(df$BsmtFinType1)] <- 'No Basement'
  df$BsmtFinType2[is.na(df$BsmtFinType2)] <- 'No Basement'
  df$MasVnrType[is.na(df$MasVnrType)] <- 'Missing Masonry Veneer'
  df$Electrical[is.na(df$Electrical)] <- 'Missing Electrical'
  return(df)
}
```

```{r}
housing <- clean_data(housing)
table(housing$Utilities)
```

```{r}
#housing$Utilities <- NULL
```


```{r}
na_count <- colSums(is.na(housing))/nrow(housing)
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs after cleaning")

```

```{r}
impute_data <- function(df){
  ## Convert character columns to factors
  df_imputed <- as.data.frame(unclass(df))
  ## ExterQual, ExterCond, HeatingQC, KitchenQual are ordinal factors
  df$ExterQual <- ordered(df$ExterQual,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$ExterCond <- ordered(df$ExterCond,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$HeatingQC <- ordered(df$HeatingQC,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$KitchenQual <- ordered(df$ExterCond,
                            levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  ## Removing the data which has no year built
  df_imputed <- df_imputed[!is.na(df_imputed$GarageYrBlt), ]
  ## Imputing missing data with mice
  df_cleaned <- mice(df_imputed, m=1, method='cart', printFlag=FALSE)
  df_cleaned <- mice::complete(df_cleaned)
  df_cleaned
}
housing_cleaned <- impute_data(housing)
```

```{r}
summary_area_df <- as.data.frame(unclass(summary(
  housing$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_df) <- "value"
knitr::kable(summary_area_df, caption = "MasVnrArea column statistics before imputing")
summary_area_clean_df <- as.data.frame(unclass(summary(
  housing_cleaned$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_clean_df) <- "value"
knitr::kable(summary_area_clean_df,caption = "MasVnrArea column statistics after imputing")
sd(housing$MasVnrArea[!is.na(housing$MasVnrArea)])
sd(housing_cleaned$MasVnrArea)
```

---------------------------

Influential points(Removed once)
```{r}
X_influential = model.matrix(SalePrice ~., housing_cleaned)[,-1]
y = housing_cleaned$SalePrice
fit_influential_pts <-  lm(y ~ X_influential)
n <- nrow(X_influential)
k <- length(fit_influential_pts$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(fit_influential_pts), 
     ylab = "Standardized dfFits", xlab = "Index", 
     main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-", round(cv,3)),ylim = c(-5,5))
abline(h = cv, lty = 2)
abline(h = -cv, lty = 2)
index_influential = which(dffits(fit_influential_pts)> cv | dffits(fit_influential_pts)< -cv)
housing_cleaned = housing_cleaned[-index_influential,]
```

---------------------------------------------------------------------------
```{r}
# Checking if any column has only 1 level after removing influential points
count_uniques <- sapply(housing_cleaned, function(x){ length(unique(x)) })
count_uniques[count_uniques == 1]
table(housing_cleaned$Utilities)
```

```{r}
# As we can observe Utilities has only 1 level after removing influential points,
# so we can remove the 'Utilities' column from our data set
housing_cleaned$Utilities <- NULL
```

Check for normality

```{r}
X = model.matrix(SalePrice ~.,housing_cleaned)[,-1]
y = housing_cleaned$SalePrice
fit_normality <-  lm(y ~ X)
resi <-  residuals(fit_normality)
fitvalues <- fitted.values(fit_normality)
plot(fitvalues,resi)
stdresi <- scale(resi)
qqnorm(stdresi, ylab="Residuals")
qqline(stdresi)
ks.test(stdresi, rnorm(length(stdresi)))
boxcox(fit_normality)
```

Check for normality after transformation
```{r}
ylog = ((y^0.5 )-1)/0.5
fit_normality_trans <-  lm(ylog ~ X)
resi <-  residuals(fit_normality_trans)
fitvalues_trans <- fitted.values(fit_normality_trans)
plot(fitvalues_trans,resi)
stdresi <- scale(resi)
qqnorm(stdresi, ylab="Residuals")
qqline(stdresi)
ks.test(stdresi, rnorm(length(stdresi)))
```

Lasso Variable Selection
```{r}
X_lasso = X[-nrow(housing_cleaned),]
y_lasso = housing_cleaned$SalePrice[-nrow(housing_cleaned)]
grid.lambda <- 10^seq(10, -2, length = 1000)
fit_lasso <- glmnet(X, y, alpha = 1, lambda = grid.lambda)

# cv.out <- cv.glmnet(X, y, alpha = 1)
# best.lambda <- cv.out$lambda.min
# plot(cv.out)
# abline(v = log(best.lambda), col = "blue", lwd = 2)
# 
# lasso.model_best.lambda = glmnet(X, y, alpha = 1, lambda = best.lambda)
# cf = coef(lasso.model_best.lambda)
# 
# # New Data Matrix
# index_nonzeroCoeff = which(cf!=0) -1
# non_zeroCoef_names = cf@Dimnames[[1]][which(cf!=0)]
# X = X[,index_nonzeroCoeff]    
# 
# #Fitting OLS Model
# ols.fit = lm(y~X)
# ols.fit$coefficients
# 
# #checking normality
# qqnorm(residuals(ols.fit), ylab="Residuals")
# qqline(residuals(ols.fit))
# shapiro.test(ols.fit$residuals)
# 
# #s = summary(ols.fit)
# significant_predictors = data.frame(summary(ols.fit)$coef[summary(ols.fit)$coef[,4] <= .05, 4])

```

## Prediction - Ridge
```{r}

diff_Morty = numeric(50)
MSPE = numeric(50)
lambda_min= numeric(50)

XMorty = X[-c(1:(nrow(X)-2)),]
X = X[-nrow(X),]

yMorty = ylog[length(ylog)]
ylog = ylog[-length(ylog)]


grid=10^seq(10,-2,length=100)

#10 Fold Validation Ridge

test <- sample(1:nrow(X), nrow(X)/4)
train <- (-test)
y.train <- ylog[train]
y.test <- ylog[test]


for(i in 1:50){
ridge.mod=glmnet(X[train,],y.train,alpha=0,lambda=grid)

cv.out <- cv.glmnet(X[train,], y.train, alpha = 0)
best.lambda <- cv.out$lambda.min
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

ridge.pred=predict(ridge.mod,s=best.lambda,newx=X[test,])
mean((ridge.pred-y.test)^2)

ridge.pred_morty=predict(ridge.mod,s=best.lambda,newx=XMorty)
Morty_SalePrice = 143000
Morty_Predicted = (ridge.pred_morty*0.5+1)^2
diff_Morty[i] = Morty_SalePrice - Morty_Predicted[2]
residuals = (y.test*0.5+1)^2 - (ridge.pred*0.5+1)^2
MSPE[i] = mean(residuals^2)
# hist(residuals)
lambda_min[i] = best.lambda
}
plot(MSPE)
lambda_min
diff_Morty

```



Multicollinearity

```{r}
# x_cont <- X[,setdiff(1:70, grep("\\d", colnames(X)))]
# x_svd <- svd(scale(x_cont))
# singular_values <- x_svd$d
# V <- x_svd$V
# tau_values <- max(singular_values)/singular_values
# high_mc_indices <- which(tau_values > 30)
# count <- 0
# sum_vj <- c()
# for(j in 1:nrow(X)){
#   for (l in 1:ncol(X)){
#   sum_vj <- V[j][l]
#   }
# }
# for(k in high_mc_indices){
#   sum <- 0
#   for(j in 1:ncol(X)){
#       pi <- (V[j][k]^2)/(singular_values[k]^2)
#       sum <- sum + /singular_values[j]^2
# }}
#X$y <- y
#vif(lm(y ~ ., data = X)) ## Detecting perfect multicollinearity
```



