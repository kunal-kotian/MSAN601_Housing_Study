---
title: "MSAN 601 Case Study"
author: "Alvira Swalin, Kunal Kotian, Sooraj Subrahmannian, Vinay Patlolla"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include = FALSE, echo = TRUE}
# Prevent comments from running off the page (force line-wrapping for comments)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```


```{r message=F}
# Load required packages
library(tidyverse)
library(magrittr)
library(glmnet)
library(ISLR)
library(mice)
library(MASS)
```

# Cleaning the Dataset

```{r}
# Load housing data
housing <- read.csv('housing.txt', stringsAsFactors = FALSE)
housing <- tbl_df(housing)
```

-----------------------------------------------------------
Vinay


```{r}
na_count <- colSums(is.na(housing))/nrow(housing)
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs before cleaning")
```

##Cleaning Data

```{r}
housing$Id <- NULL
housing$PoolQC[is.na(housing$PoolQC)] <- 'No Pool'
housing$Alley[is.na(housing$Alley)] <- 'No Alley'
housing$MiscFeature[is.na(housing$MiscFeature)] <- 'None'
housing$Fence[is.na(housing$Fence)] <- 'No Fence'
housing$FireplaceQu[is.na(housing$FireplaceQu)] <- 'No Fireplace'
#housing$LotFrontage[is.na(housing$LotFrontage)] <- 'No Lot'
housing$GarageType[is.na(housing$GarageType)] <- 'No Garage'
#housing$GarageYrBlt[is.na(housing$GarageYrBlt)] <- 'No Garage'
#housing$GarageFinish[is.na(housing$GarageFinish)] <- 'No Garage'
housing$GarageQual[is.na(housing$GarageQual)] <- 'No Garage'
housing$GarageCond[is.na(housing$GarageCond)] <- 'No Garage'
housing$BsmtExposure[is.na(housing$BsmtExposure)] <- 'No Basement'
housing$BsmtQual[is.na(housing$BsmtQual)] <- 'No Basement'
housing$BsmtCond[is.na(housing$BsmtCond)] <- 'No Basement'
housing$BsmtFinType1[is.na(housing$BsmtFinType1)] <- 'No Basement'
housing$BsmtFinType2[is.na(housing$BsmtFinType2)] <- 'No Basement'
housing$MasVnrType[is.na(housing$MasVnrType)] <- 'Missing Masonry Veneer'
housing$Electrical[is.na(housing$Electrical)] <- 'Missing Electrical'
```

```{r}
table(housing$Utilities)
```
```{r}
housing$Utilities <- NULL
```


```{r}
na_count <- colSums(is.na(housing))/nrow(housing)
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs after cleaning")

```

```{r}
## Convert character columns to factors
housing_imputed <- as.data.frame(unclass(housing))
## Removing the data which has no year built
housing_imputed <- housing_imputed[!is.na(housing_imputed$GarageYrBlt), ]
## Im
housing_imputed_mice <- mice(housing_imputed, m=1, method='cart', printFlag=FALSE)
housing_imputed_mice <- mice::complete(housing_imputed_mice)
summary(housing_imputed$MasVnrArea[!is.na(housing_imputed$MasVnrArea)])
summary(housing_imputed_mice$MasVnrArea)
sd(housing_imputed$MasVnrArea[!is.na(housing_imputed$MasVnrArea)])
sd(housing_imputed_mice$MasVnrArea)
housing_cleaned = housing_imputed_mice
```

---------------------------------------------------------------------------
Test for normality

```{r}
# Fitting an OLS model


# housing_imputed_mice1 <- housing_imputed_mice[, sapply(housing_imputed_mice, nlevels) > 1]
# 
# housing_imputed_mice2 <- housing_imputed_mice[, sapply(housing_imputed_mice, nlevels) <= 1]

fit_for_normality <- lm(SalePrice ~ ., data = housing_cleaned)
qqnorm(residuals(fit_for_normality), ylab="Residuals")
qqline(residuals(fit_for_normality))
boxcox(fit_for_normality) ##Since the 95% CI is closest to zero, log of Y is taken for transformation
shapiro.test(fit_for_normality$residuals)
housing_cleaned$SalePrice = log(housing_cleaned$SalePrice)
housing_cleaned = na.omit(housing_cleaned)
housing_cleaned = housing_cleaned[,c(-1,-10)]
```


-----------------------------------------------------------

Alvira

# Removing Influential Obervations
```{r}

X = model.matrix(SalePrice ~.,housing_cleaned)[,-1]
Xtest = X[nrow(housing_cleaned),]
X = X[-nrow(housing_cleaned),]
y = housing_cleaned$SalePrice[-nrow(housing_cleaned)]
yMorty = housing_cleaned$SalePrice[nrow(housing_cleaned)]

ols_model <- lm(y~X)
cooksd <- cooks.distance(ols_model)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
upper_limit_cooksd = 4*mean(cooksd, na.rm = T)
index_influential = which(cooksd > upper_limit_cooksd)
len_influential = length(index_influential)
housing_cleaned = housing_cleaned[-index_influential,]
```


```{r}


X = model.matrix(SalePrice ~.,housing_cleaned)[,-1]

X = X[-nrow(housing_cleaned),]
y = housing_cleaned$SalePrice[-nrow(housing_cleaned)]



grid.lambda <- 10^seq(10, -2, length = 100)

#10 Fold Validation Lasso Selection
# train <- sample(1:nrow(X), nrow(X) / 2)
# test <- (-train)
# y.train <- y[train]
# y.test <- y[test]
lasso.model.train <- glmnet(X, y, alpha = 1, lambda = grid.lambda)

cv.out <- cv.glmnet(X, y, alpha = 1)
best.lambda <- cv.out$lambda.min
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

lasso.model_best.lambda = glmnet(X, y, alpha = 1, lambda = best.lambda)
cf = coef(lasso.model_best.lambda)

# New Data Matrix
index_nonzeroCoeff = which(cf!=0) -1
non_zeroCoef_names = cf@Dimnames[[1]][which(cf!=0)]
X = X[,index_nonzeroCoeff]    

#Fitting OLS Model
ols.fit = lm(y~X)
ols.fit$coefficients

#checking normality
qqnorm(residuals(ols.fit), ylab="Residuals")
qqline(residuals(ols.fit))
shapiro.test(ols.fit$residuals)

#s = summary(ols.fit)
significant_predictors = data.frame(summary(ols.fit)$coef[summary(ols.fit)$coef[,4] <= .05, 4])

```

## Prediction - Ridge
```{r}


X = model.matrix(SalePrice ~.,housing_cleaned)[,-1]
XMorty = X[-c(1:(nrow(X)-2)),]
X = X[-nrow(housing_cleaned),]
y = housing_cleaned$SalePrice[-nrow(housing_cleaned)]
yMorty = housing_cleaned$SalePrice[nrow(housing_cleaned)]

grid=10^seq(10,-2,length=100)


#10 Fold Validation Ridge

train <- sample(1:nrow(X), nrow(X)/2)
test <- (-train)
y.train <- y[train]
y.test <- y[test]

ridge.mod=glmnet(X[train,],y[train],alpha=0,lambda=grid)

cv.out <- cv.glmnet(X[train,], y.train, alpha = 0)
best.lambda <- cv.out$lambda.min
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

ridge.pred=predict(ridge.mod,s=best.lambda,newx=X[test,])
mean((ridge.pred-y.test)^2)

ridge.pred_morty=predict(ridge.mod,s=best.lambda,newx=XMorty)
Morty_SalePrice = 143000
Morty_Predicted = exp(ridge.pred_morty)
diff = Morty_SalePrice - Morty_Predicted[2]
diff


```



Multicollinearity

```{r}
# x_cont <- X[,setdiff(1:70, grep("\\d", colnames(X)))]
# x_svd <- svd(scale(x_cont))
# singular_values <- x_svd$d
# V <- x_svd$V
# tau_values <- max(singular_values)/singular_values
# high_mc_indices <- which(tau_values > 30)
# count <- 0
# sum_vj <- c()
# for(j in 1:nrow(X)){
#   for (l in 1:ncol(X)){
#   sum_vj <- V[j][l]
#   }
# }
# for(k in high_mc_indices){
#   sum <- 0
#   for(j in 1:ncol(X)){
#       pi <- (V[j][k]^2)/(singular_values[k]^2)
#       sum <- sum + /singular_values[j]^2
# }}
X$y <- y
vif(lm(y ~ ., data = X)) ## Detecting perfect multicollinearity
```



