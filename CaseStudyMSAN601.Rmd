---
title: "MSAN 601 Case Study"
author: "Alvira Swalin, Kunal Kotian, Sooraj Subrahmannian, Vinay Patlolla"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include = FALSE, echo = TRUE}
# Prevent comments from running off the page (force line-wrapping for comments)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```

# Introduction

This report focuses on the analysis of the value of residential homes in Ames, Iowa.  Linear regression techniques were used to address the following key questions:

-	What aspects of a house in Ames, IA most strongly influence its value?
-	How can we predict house prices in Ames, IA?

The report is divided into 3 main sections.  Section 1 describes the process of cleaning the housing data, Section 2 describes how an explanatory model was built for the housing data, and finally Section 3 focuses on predictive modeling for estimating housing prices.

```{r message=F, warning=F, echo=F}
# Load required packages
library(tidyverse)
library(magrittr)
library(glmnet)
library(ISLR)
library(mice)
library(MASS)
library(car)
library(VIM)
library(corrplot)
set.seed(1)

```

# Cleaning the Housing Dataset

```{r}
# Load housing data
housing <- read.csv('housing.txt', stringsAsFactors = FALSE)
Mortydata <- read.csv('Morty.txt',stringsAsFactors = F)
housing <- rbind(housing,Mortydata[,-1])
housing <- tbl_df(housing)
```

-----------------------------------------------------------
Vinay


```{r}
# Counting ratio of NA's in each column
na_count <- colSums(is.na(housing))/nrow(housing)
# Summarising the columns which have NA count > 0
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs before cleaning")
mice_plot <- aggr(housing, col=c('navyblue','red'),
                    numbers=T, sortVars=TRUE,
                    labels=names(housing), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"),combined =F)
```

##Cleaning Data

```{r}
# Defining function clean data which cleans the dataset
# based on the description
# For example: In Basement Quality column, NA represents 'No Basement' which is represented
# as a new level for that category. In cases like MasVnrType column, where the data is actually
# we defined the level as 'Missing Veneer Type'
clean_data <- function(df){
  df$Id <- NULL
  df$PoolQC[is.na(df$PoolQC)] <- 'No Pool'
  df$Alley[is.na(df$Alley)] <- 'No Alley'
  df$MiscFeature[is.na(df$MiscFeature)] <- 'None'
  df$Fence[is.na(df$Fence)] <- 'No Fence'
  df$FireplaceQu[is.na(df$FireplaceQu)] <- 'No Fireplace'
  #df$LotFrontage[is.na(df$LotFrontage)] <- 'No Lot'
  df$GarageType[is.na(df$GarageType)] <- 'No Garage'
  #df$GarageYrBlt[is.na(df$GarageYrBlt)] <- 'No Garage'
  #df$GarageFinish[is.na(df$GarageFinish)] <- 'No Garage'
  df$GarageQual[is.na(df$GarageQual)] <- 'No Garage'
  df$GarageCond[is.na(df$GarageCond)] <- 'No Garage'
  df$BsmtExposure[is.na(df$BsmtExposure)] <- 'No Basement'
  df$BsmtQual[is.na(df$BsmtQual)] <- 'No Basement'
  df$BsmtCond[is.na(df$BsmtCond)] <- 'No Basement'
  df$BsmtFinType1[is.na(df$BsmtFinType1)] <- 'No Basement'
  df$BsmtFinType2[is.na(df$BsmtFinType2)] <- 'No Basement'
  df$MasVnrType[is.na(df$MasVnrType)] <- 'Missing Masonry Veneer'
  df$Electrical[is.na(df$Electrical)] <- 'Missing Electrical'
  return(df)
}
```

```{r}
# Applying clean data function on housing data set
housing <- clean_data(housing)
# Summarising NA ratio after cleaning
na_count <- colSums(is.na(housing))/nrow(housing)
knitr::kable(sort(na_count[na_count > 0], decreasing = T), 
             caption = "Ratio of NAs after cleaning")

```

```{r}
# We need to impute data for missing values in columns like LotFrontage and MasVnrArea
# Also removing observations which have NA's in GarageYrBlt because NA's here represent
# that house does not have any garage and imputing here does not make sense i.e there
# is no proper way to represent these observations.
impute_data <- function(df){
  ## Convert character columns to factors
  df_imputed <- as.data.frame(unclass(df))
  # Columns(ExterQual, ExterCond, HeatingQC, KitchenQual) which have a inherent order
  # are converted to ordinal factors
  df$ExterQual <- ordered(df$ExterQual,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$ExterCond <- ordered(df$ExterCond,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$HeatingQC <- ordered(df$HeatingQC,
                          levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  df$KitchenQual <- ordered(df$ExterCond,
                            levels = c("Ex", "Gd", "TA", "Fa", "Po"))
  ## Removing the observations which have NA values in GarageYrBlt
  df_imputed <- df_imputed[!is.na(df_imputed$GarageYrBlt), ]
  ## Imputing missing data with mice using CART method
  df_cleaned <- mice(df_imputed, m=1, method='cart', printFlag=FALSE)
  df_cleaned <- mice::complete(df_cleaned)
  df_cleaned
}
housing_cleaned <- impute_data(housing)
mortydata <-housing_cleaned[nrow(housing_cleaned),]
housing_cleaned <- housing_cleaned[-nrow(housing_cleaned),]
```

```{r}
# Summarising statistics of each column before and after imputation
summary_area_df <- as.data.frame(unclass(summary(
  housing$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_df) <- "value"
knitr::kable(summary_area_df, caption = "MasVnrArea column statistics before imputing")
summary_area_clean_df <- as.data.frame(unclass(summary(
  housing_cleaned$MasVnrArea[!is.na(housing$MasVnrArea)])))
colnames(summary_area_clean_df) <- "value"
knitr::kable(summary_area_clean_df,caption = "MasVnrArea column statistics after imputing")
# Summarising sd of each column before and after imputation
sd(housing$MasVnrArea[!is.na(housing$MasVnrArea)])
sd(housing_cleaned$MasVnrArea)
```

---------------------------

Influential points(Removed once)
```{r}
X_influential = model.matrix(SalePrice ~., housing_cleaned)[,-1]
y = housing_cleaned$SalePrice
fit_influential_pts <-  lm(y ~ X_influential)
n <- nrow(X_influential)
k <- length(fit_influential_pts$coefficients)-1
cv <- 2*sqrt(k/n)
plot(dffits(fit_influential_pts), 
     ylab = "Standardized dfFits", xlab = "Index", 
     main = paste("Standardized DfFits, \n critical value = 2*sqrt(k/n) = +/-",
                  round(cv,3)),
     ylim = c(-5,5))
abline(h = cv, lty = 2)
abline(h = -cv, lty = 2)
index_influential = which(dffits(fit_influential_pts)> cv | dffits(fit_influential_pts)< -cv)
housing_cleaned = housing_cleaned[-index_influential,]
```

---------------------------------------------------------------------------

Checking for redundant columns
```{r}
# Checking if any column has only 1 level after removing influential points
count_uniques <- sapply(housing_cleaned, function(x){ length(unique(x)) })
count_uniques[count_uniques == 1]
table(housing_cleaned$Utilities)
```

```{r}
# As we can observe Utilities has only 1 level after removing influential points,
# so we can remove the 'Utilities' column from our data set
housing_cleaned$Utilities <- NULL
mortydata$Utilities <- NULL
```


Model matrix creation 
```{r}
mortyincluded <- rbind(housing_cleaned,mortydata)
X = model.matrix(SalePrice ~.,mortyincluded)[,-1]
XMorty <- X[nrow(X),]
Xtrainingmodel <- X[-nrow(X),]
dim(X)
length(XMorty)
XMorty <- t(as.data.frame(XMorty))

```



<!-- Lasso Variable Selection -->
<!-- ```{r} -->
<!-- # Lasso model fit for variable selection -->
<!-- set.seed(1) -->
<!-- X_lasso <- Xtrainingmodel -->
<!-- y_lasso <- housing_cleaned$SalePrice -->
<!-- fit_lasso <- cv.glmnet(X_lasso, y_lasso, alpha = 1) -->
<!-- best_lambda_lasso <- fit_lasso$lambda.min -->
<!-- plot(fit_lasso) -->
<!-- # Identify variables selected -->
<!-- coeffs_lasso <- coef(fit_lasso, s = "lambda.1se") -->
<!-- inds_lasso_fit <- which(coeffs_lasso != 0) -->
<!-- variables <- row.names(coeffs_lasso)[inds_lasso_fit] -->
<!-- variables<-variables[variables %in% '(Intercept)'] -->
<!-- ``` -->

Lasso for prediction
```{r}

X_lasso <- model.matrix(SalePrice ~.,housing_cleaned)[,-1]

# XMorty <- X_lasso[nrow(X_lasso), , drop = FALSE]
X_lasso <- Xtrainingmodel
yMorty <- mortydata$SalePrice
y_lasso <- housing_cleaned$SalePrice
grid.lambda <- 10^seq(10, -2, length = 1000)


cv.out <- cv.glmnet(X_lasso, y_lasso, alpha = 1)
best.lambda <- cv.out$lambda.1se
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

lasso.model_best.lambda <- glmnet(X_lasso, y_lasso, alpha = 1, lambda = best.lambda)
cf <- coef(lasso.model_best.lambda)

# New Data Matrix
col_names <- names(cf[-1,])[which(cf[-1,] != 0)]
X_lasso <- X_lasso[, col_names]
XMorty <- XMorty[, col_names, drop = F]

myDf <- data.frame(y_lasso = y_lasso, X_lasso)
#Fitting OLS Model
ols.fit <- lm(y_lasso~., data=myDf)
#ols.fit$coefficients
Y_pred_Morty <- predict(ols.fit, data.frame(XMorty), interval="confidence", level = 0.95)
```

Check for normality before tranformation
```{r}
resi <- residuals(ols.fit)
fitvalues <- fitted.values(ols.fit)
plot(fitvalues, resi)
#abline(, col = "blue", lwd = 2)
stdresi <- scale(resi)
qqnorm(stdresi, ylab="Residuals")
qqline(stdresi)
ks.test(stdresi, rnorm(length(stdresi)))
boxcox(ols.fit)
```

Check for normality after transformation
```{r}
y_sqrt = (y_lasso^0.5)
fit_normality_trans <- lm(y_sqrt ~ X_lasso)
resi <- residuals(fit_normality_trans)
fitvalues_trans <- fitted.values(fit_normality_trans)
fitdf <- data.frame(cbind(fitvalues_trans, resi))
resiplot<- lm(fitvalues_trans ~ resi, data = fitdf)
plot(fitvalues_trans,resi)
abline(resiplot, col = "red")
stdresi <- scale(resi)
qqnorm(stdresi, ylab="Residuals")
qqline(stdresi)
ks.test(stdresi, rnorm(length(stdresi)))
```

```{r}
myDf <- data.frame(y_sqrt = y_sqrt, X_lasso)
#Fitting OLS Model
ols.fit <- lm(y_sqrt~., data=myDf)

significant_predictors <- data.frame(summary(ols.fit)$coef[summary(ols.fit)$coef[,4] <= .05, 4])
X_lasso_significant <- X_lasso[, rownames(significant_predictors)[-1]]
```

Plotting pairwise collinearity
```{r fig1, fig.cap = "Pairwise correlation plot", fig.height = 10, fig.width = 10}
pairwise_cors <- cor(X_lasso)
corrplot(pairwise_cors)
vif_scores <- vif(ols.fit)
vif_scores <- data.frame(vif(ols.fit))
vif_scores['predictors'] <- rownames(vif_scores)
rownames(vif_scores) <- NULL
vif_scores <- vif_scores[, c(2, 1)]
colnames(vif_scores) <- c('predictors', 'scores')
knitr::kable(vif_scores[order(-vif_scores$scores), ],
             caption = "VIF scores of significant predictors")
```


## Prediction - Ridge
```{r}

X <- model.matrix(SalePrice ~.,housing_cleaned)[,-1]

y <- housing_cleaned$SalePrice


diff_Morty <- matrix(0, nrow = 10, ncol = 100)
MSPE <- matrix(0, nrow = 10,ncol = 100)
lambda_min <- matrix(0, nrow = 10, ncol = 100)

XMorty <- X[-c(1:(nrow(X) - 2)),]
X <- X[-nrow(X),]

yMorty <- y[length(y)]
y <- y[-length(y)]


grid <- 10^seq(10, -2, length = 100)

#10 Fold Validation Ridge
j <- 0
for (j in 1:10) {
  alpha <- j/10
for(i in 1:10)
{
set.seed(i)
test <- sample(1:nrow(X), nrow(X)/5)
train <- (-test)
y.train <- y[train]
y.test <- y[test]
  
ridge.mod <- glmnet(X[train,], y.train, alpha = alpha)

cv.out <- cv.glmnet(X[train,], y.train, alpha = alpha)
best.lambda <- cv.out$lambda.min
# plot(cv.out)
# abline(v = log(best.lambda), col = "blue", lwd = 2)

ridge.pred <- predict(ridge.mod,s=best.lambda,newx=X[test,])
mean((ridge.pred-y.test)^2)

# ridge.pred_morty=predict(ridge.mod,s=best.lambda,newx=XMorty)
# Morty_SalePrice = 143000
# diff_Morty[j,i] = Morty_SalePrice - ridge.pred_morty[2]

residuals <- y.test - ridge.pred
MSPE[j,i] <- mean(residuals^2)
# hist(residuals)
lambda_min[j,i] <- best.lambda
}
}
# plot(MSPE)
# diff_Morty

```



Multicollinearity

```{r}
# x_cont <- X[,setdiff(1:70, grep("\\d", colnames(X)))]
# x_svd <- svd(scale(x_cont))
# singular_values <- x_svd$d
# V <- x_svd$V
# tau_values <- max(singular_values)/singular_values
# high_mc_indices <- which(tau_values > 30)
# count <- 0
# sum_vj <- c()
# for(j in 1:nrow(X)){
#   for (l in 1:ncol(X)){
#   sum_vj <- V[j][l]
#   }
# }
# for(k in high_mc_indices){
#   sum <- 0
#   for(j in 1:ncol(X)){
#       pi <- (V[j][k]^2)/(singular_values[k]^2)
#       sum <- sum + /singular_values[j]^2
# }}
#X$y <- y
#vif(lm(y ~ ., data = X)) ## Detecting perfect multicollinearity
```



